{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aea8764-1f2e-4f71-af32-621a6cc587d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb65a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e882d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascade=cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    if ret ==False:\n",
    "        continue\n",
    "    # Finding All the faces in the frame\n",
    "    faces=face_cascade.detectMultiScale(frame,1.3,5)  # Frame , scaling factor , neighbors\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3) # Frame , start pos , end pos , color , thickness\n",
    "        \n",
    "        \n",
    "    cv2.imshow(\"video frame\",frame)\n",
    "    \n",
    "    #wait for user to stop\n",
    "    key_pressed=cv2.waitKey(1) & 0xFF\n",
    "    if key_pressed==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8a2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c56eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your name:- daniyal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(25, 100, 100, 3)\n",
      "(25, 30000)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/daniyal.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m face_data \u001b[38;5;241m=\u001b[39m face_data\u001b[38;5;241m.\u001b[39mreshape((face_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(face_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mface_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     47\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()  \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/daniyal.npy'"
     ]
    }
   ],
   "source": [
    "name = input(\"Enter your name:-\")\n",
    "num = 25 #number of frame will be taken\n",
    "\n",
    "face_data = []\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(\"daniyal.mp4\")\n",
    "\n",
    "\n",
    "face_cascade=cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "while True and num:\n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    if ret ==False:\n",
    "        continue\n",
    "    # Find All the faces in the frame\n",
    "    faces=face_cascade.detectMultiScale(frame,1.3,5)  # Frame , scaling factor , neighbors\n",
    "    faces=sorted(faces,key=lambda x:x[2]*x[3],reverse=True)\n",
    "    faces=faces[:1]\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_selection = frame[y:y+h , x:x+w] #area of intrest\n",
    "        cv2.imshow(\"Face_selection\", face_selection)\n",
    "        face_selection = cv2.resize(face_selection,(100,100))\n",
    "        print(face_selection.shape)\n",
    "        face_data.append(face_selection)\n",
    "        \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3) # Frame , start pos , end pos , color , thickness\n",
    "        \n",
    "        num-=1\n",
    "        \n",
    "    cv2.imshow(\"video frame\",frame)\n",
    "    \n",
    "    #wait for user to stop\n",
    "    key_pressed=cv2.waitKey(1) & 0xFF\n",
    "    if key_pressed==ord('q'):\n",
    "        break\n",
    "        \n",
    "        \n",
    "face_data = np.array(face_data)\n",
    "print(face_data.shape)\n",
    "\n",
    "#convert face list array into numpy array\n",
    "face_data = face_data.reshape((face_data.shape[0],-1))\n",
    "print(face_data.shape)\n",
    "np.save((\"/\"+name ),face_data)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49202a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0443a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd606c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "(300, 30000)\n",
      "(300, 30000)\n",
      "(2, 300, 30000)\n",
      "[0 1]\n",
      "(600, 30000)\n",
      "(600,)\n",
      "(600, 1)\n",
      "(600, 30001)\n"
     ]
    }
   ],
   "source": [
    "files = [faces for faces in  os.listdir('datasets') if faces.endswith('.npy') ]\n",
    "names = [faces[:-4] for faces in files]\n",
    "label_encoder = LabelEncoder()\n",
    "names = label_encoder.fit_transform(names)\n",
    "\n",
    "print(names)\n",
    "\n",
    "face_data = []\n",
    "\n",
    "for filename in files:\n",
    "    data = np.load('datasets/' + filename)\n",
    "    print(data.shape)\n",
    "    face_data.append(data)\n",
    "\n",
    "face_data=np.array(face_data)\n",
    "print(face_data.shape)\n",
    "print(names)\n",
    "face_data = np.concatenate(face_data ,axis = 0)\n",
    "print(face_data.shape)\n",
    "names = np.repeat(names ,300)\n",
    "print(names.shape)\n",
    "names = names.reshape((names.shape[0],1))\n",
    "print(names.shape)\n",
    "# dataset = np.hstack((face_data, names))\n",
    "dataset = np.hstack((face_data, names.reshape(-1, 1)))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e89cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_pred=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53cbdfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_pred.fit(dataset[:, :-1], dataset[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31593d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "[[ 27 606 513 513]]\n",
      "(513, 513, 3)\n",
      "(100, 100, 3)\n",
      "(1, 30000)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "[[ 16 602 466 466]]\n",
      "(466, 466, 3)\n",
      "(100, 100, 3)\n",
      "(1, 30000)\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture('hamzah_daniyal.mp4') #video location or 0 for video stream from webcam\n",
    "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "\n",
    "while True:\n",
    "    ret , frame = cap.read() # Status , Frame\n",
    "    if not ret:\n",
    "        continue\n",
    "    # Find All the faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(frame , 1.3 ,5) # Frame , scaling factor , neighbors\n",
    "\n",
    "        \n",
    "    print(faces)\n",
    "\n",
    "    for face in faces:\n",
    "        x,y,w,h = face\n",
    "        face_selection = frame[y:y+h , x:x+w]\n",
    "        print(face_selection.shape)\n",
    "        # cv2.imshow(\"Face_selection\", face_selection)\n",
    "        face_selection = cv2.resize(face_selection,(100,100))\n",
    "        print(face_selection.shape)\n",
    "        face_cropped = face_selection.reshape((1,-1))\n",
    "        print(face_cropped.shape)\n",
    "        \n",
    "        #pred=face_pred.predict(face_cropped)\n",
    "        pred = face_pred.predict(face_cropped)\n",
    "        pred = label_encoder.inverse_transform(pred)\n",
    "        \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),5) # Frame , start pos , end pos , color , thickness\n",
    "        cv2.putText(frame, pred[0], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Feed\" , frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3463d-972a-4ec6-84ff-6b4a72e2f332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
